Possible enhancements:

1. It should be possible to limit the number of requests not only by the number of requests per second but also
by the number of concurrent active requests, i.e. maxConcurrentRequests: 5

This should provide yet another sensible approach to limiting the bandwidth being used up.

2. In case there is an error when crawling some url, this url can be queued again, and only fully abandoned from crawling after another 2 repeated failures

3. Normalize the urls, so that urls https://github.com https://github.com/ are considered to be the same url

4. Re-factor the crawler, add unit tests and the build infrastructure

5. Look at and test more the API for forgetting crawled urls

6. Better intuitive API for crawling several urls